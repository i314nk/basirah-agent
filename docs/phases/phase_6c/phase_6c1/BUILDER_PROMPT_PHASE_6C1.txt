# BUILDER PROMPT: Phase 6C.1 - Analysis History with Docker Database & Search

**Project:** basƒ´rah Warren Buffett AI Agent
**Phase:** 6C.1 - Analysis History, Storage & Search
**Priority:** HIGH (Foundation for Batch Processing)
**Estimated Time:** 6-7 hours

---

## OVERVIEW

Phase 6C.1 adds a comprehensive analysis history system with:

1. **Docker Database** - PostgreSQL database running in Docker container
2. **Smart Storage** - Organized directory structure + database indexing
3. **Powerful Search** - Multi-criteria search with filters and sorting
4. **History Browser** - Beautiful UI for browsing past analyses
5. **Statistics Dashboard** - Insights about your analysis history

This is the foundation that enables batch processing (Phase 6C.2).

---

## ARCHITECTURE OVERVIEW

```
basƒ´rah Architecture (Phase 6C.1)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Streamlit UI (app.py)               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Analysis   ‚îÇ  History Browser         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Interface  ‚îÇ  - Search                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ            ‚îÇ  - Filters               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ            ‚îÇ  - Statistics            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Storage Layer (analysis_storage.py)     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Database Manager ‚îÇ  File Manager     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ (PostgreSQL)     ‚îÇ  (JSON files)     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgreSQL ‚îÇ    ‚îÇ File System      ‚îÇ
‚îÇ (Docker)   ‚îÇ    ‚îÇ basƒ´rah_analyses/‚îÇ
‚îÇ            ‚îÇ    ‚îÇ ‚îú‚îÄ deep_dive/    ‚îÇ
‚îÇ - Metadata ‚îÇ    ‚îÇ ‚îú‚îÄ quick_screen/ ‚îÇ
‚îÇ - Indexes  ‚îÇ    ‚îÇ ‚îî‚îÄ sharia_screen/‚îÇ
‚îÇ - Search   ‚îÇ    ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Why This Architecture:**
- **Database:** Fast search, complex queries, relationships
- **File System:** Full analysis content, easy to backup
- **Docker:** Consistent environment, easy deployment
- **Hybrid:** Best of both worlds (speed + flexibility)

---

## DATABASE SETUP (DOCKER)

### Step 1: Create Docker Compose Configuration

**File:** Create new file `docker-compose.yml` in project root

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:16-alpine
    container_name: basirah_db
    restart: unless-stopped
    environment:
      POSTGRES_DB: basirah
      POSTGRES_USER: basirah_user
      POSTGRES_PASSWORD: ${DB_PASSWORD:-basirah_secure_password_2025}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U basirah_user -d basirah"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
    driver: local
```

### Step 2: Create Database Schema

**File:** Create new file `db/init/01_create_schema.sql`

```sql
-- basƒ´rah Analysis Storage Schema
-- PostgreSQL 16+

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm"; -- For fuzzy text search

-- Companies table (master list)
CREATE TABLE companies (
    id SERIAL PRIMARY KEY,
    ticker VARCHAR(10) NOT NULL UNIQUE,
    company_name VARCHAR(255) NOT NULL,
    sector VARCHAR(100),
    industry VARCHAR(100),
    first_analyzed TIMESTAMP NOT NULL DEFAULT NOW(),
    last_analyzed TIMESTAMP NOT NULL DEFAULT NOW(),
    total_analyses INTEGER NOT NULL DEFAULT 0,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Analyses table (main storage)
CREATE TABLE analyses (
    id SERIAL PRIMARY KEY,
    analysis_id VARCHAR(255) NOT NULL UNIQUE, -- e.g., "AAPL_2025-11-04_buy_10y"
    company_id INTEGER NOT NULL REFERENCES companies(id) ON DELETE CASCADE,
    ticker VARCHAR(10) NOT NULL,
    company_name VARCHAR(255) NOT NULL,
    
    -- Analysis metadata
    analysis_type VARCHAR(50) NOT NULL, -- 'quick', 'deep_dive', 'sharia'
    analysis_date DATE NOT NULL,
    analysis_datetime TIMESTAMP NOT NULL DEFAULT NOW(),
    years_analyzed INTEGER, -- For deep dive (1-10)
    
    -- Results
    decision VARCHAR(50) NOT NULL, -- 'BUY', 'WATCH', 'AVOID', 'INVESTIGATE', 'PASS', etc.
    conviction VARCHAR(50), -- 'HIGH', 'MODERATE', 'LOW'
    
    -- Financial metrics (for deep dive)
    intrinsic_value DECIMAL(12, 2),
    current_price DECIMAL(12, 2),
    margin_of_safety DECIMAL(5, 2), -- Percentage
    roic DECIMAL(5, 2), -- Percentage
    
    -- Sharia metrics
    sharia_status VARCHAR(50), -- 'COMPLIANT', 'DOUBTFUL', 'NON-COMPLIANT'
    purification_rate DECIMAL(5, 2), -- Percentage
    
    -- Cost and performance
    cost DECIMAL(6, 2) NOT NULL,
    duration_seconds INTEGER NOT NULL,
    token_usage_input INTEGER,
    token_usage_output INTEGER,
    
    -- Content
    thesis_preview TEXT, -- First 500 chars for quick display
    thesis_full TEXT, -- Full thesis content
    
    -- File storage
    file_path VARCHAR(500) NOT NULL, -- Relative path to JSON file
    
    -- Metadata
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
    
    -- Constraints
    CHECK (analysis_type IN ('quick', 'deep_dive', 'sharia')),
    CHECK (margin_of_safety IS NULL OR (margin_of_safety >= -100 AND margin_of_safety <= 100)),
    CHECK (roic IS NULL OR roic >= 0),
    CHECK (purification_rate IS NULL OR (purification_rate >= 0 AND purification_rate <= 100))
);

-- Tags table (for custom categorization)
CREATE TABLE tags (
    id SERIAL PRIMARY KEY,
    name VARCHAR(50) NOT NULL UNIQUE,
    description TEXT,
    color VARCHAR(7), -- Hex color code
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Analysis tags (many-to-many)
CREATE TABLE analysis_tags (
    analysis_id INTEGER NOT NULL REFERENCES analyses(id) ON DELETE CASCADE,
    tag_id INTEGER NOT NULL REFERENCES tags(id) ON DELETE CASCADE,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    PRIMARY KEY (analysis_id, tag_id)
);

-- Saved searches
CREATE TABLE saved_searches (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    search_criteria JSONB NOT NULL, -- Store filter criteria as JSON
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    last_used TIMESTAMP
);

-- Analysis comparison history
CREATE TABLE comparisons (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    analysis_ids INTEGER[] NOT NULL, -- Array of analysis IDs
    notes TEXT,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Indexes for fast queries
CREATE INDEX idx_analyses_ticker ON analyses(ticker);
CREATE INDEX idx_analyses_date ON analyses(analysis_date DESC);
CREATE INDEX idx_analyses_type ON analyses(analysis_type);
CREATE INDEX idx_analyses_decision ON analyses(decision);
CREATE INDEX idx_analyses_conviction ON analyses(conviction);
CREATE INDEX idx_analyses_sharia ON analyses(sharia_status);
CREATE INDEX idx_analyses_company ON analyses(company_id);
CREATE INDEX idx_analyses_composite ON analyses(analysis_type, decision, analysis_date DESC);

-- Full-text search indexes
CREATE INDEX idx_companies_name_trgm ON companies USING gin(company_name gin_trgm_ops);
CREATE INDEX idx_analyses_thesis_trgm ON analyses USING gin(thesis_full gin_trgm_ops);

-- Function to update company statistics
CREATE OR REPLACE FUNCTION update_company_stats()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        UPDATE companies 
        SET 
            last_analyzed = NEW.analysis_datetime,
            total_analyses = total_analyses + 1,
            updated_at = NOW()
        WHERE id = NEW.company_id;
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Trigger to auto-update company stats
CREATE TRIGGER trigger_update_company_stats
AFTER INSERT ON analyses
FOR EACH ROW
EXECUTE FUNCTION update_company_stats();

-- Function to update timestamps
CREATE OR REPLACE FUNCTION update_updated_at()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Triggers for updated_at
CREATE TRIGGER trigger_companies_updated_at
BEFORE UPDATE ON companies
FOR EACH ROW
EXECUTE FUNCTION update_updated_at();

CREATE TRIGGER trigger_analyses_updated_at
BEFORE UPDATE ON analyses
FOR EACH ROW
EXECUTE FUNCTION update_updated_at();

-- Insert default tags
INSERT INTO tags (name, description, color) VALUES
    ('Portfolio', 'Companies in my portfolio', '#10B981'),
    ('Watchlist', 'Companies to monitor', '#F59E0B'),
    ('High Priority', 'Urgent analysis needed', '#EF4444'),
    ('Halal', 'Sharia compliant companies', '#8B5CF6'),
    ('Re-screen', 'Needs updated analysis', '#3B82F6'),
    ('Archived', 'Old analysis, no longer relevant', '#6B7280');

-- Create view for easy querying
CREATE VIEW v_analysis_summary AS
SELECT 
    a.id,
    a.analysis_id,
    a.ticker,
    a.company_name,
    c.sector,
    c.industry,
    a.analysis_type,
    a.analysis_date,
    a.years_analyzed,
    a.decision,
    a.conviction,
    a.intrinsic_value,
    a.current_price,
    a.margin_of_safety,
    a.roic,
    a.sharia_status,
    a.purification_rate,
    a.cost,
    a.duration_seconds,
    a.thesis_preview,
    a.file_path,
    a.created_at,
    ARRAY_AGG(t.name) FILTER (WHERE t.name IS NOT NULL) as tags
FROM analyses a
LEFT JOIN companies c ON a.company_id = c.id
LEFT JOIN analysis_tags at ON a.id = at.analysis_id
LEFT JOIN tags t ON at.tag_id = t.id
GROUP BY a.id, c.sector, c.industry;

-- Grant permissions
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO basirah_user;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO basirah_user;
GRANT ALL PRIVILEGES ON ALL FUNCTIONS IN SCHEMA public TO basirah_user;
```

### Step 3: Database Connection Manager

**File:** Create new file `src/storage/database.py`

```python
"""
Database connection and management for basƒ´rah.
Uses PostgreSQL running in Docker.
"""

import os
import logging
from typing import Optional
import psycopg2
from psycopg2.extras import RealDictCursor
from psycopg2.pool import SimpleConnectionPool
from contextlib import contextmanager
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)


class DatabaseManager:
    """
    Manages PostgreSQL database connections using connection pooling.
    """
    
    def __init__(self):
        """Initialize database connection pool."""
        self.pool: Optional[SimpleConnectionPool] = None
        self._initialize_pool()
    
    def _initialize_pool(self):
        """Create connection pool."""
        try:
            self.pool = SimpleConnectionPool(
                minconn=1,
                maxconn=10,
                host=os.getenv("DB_HOST", "localhost"),
                port=os.getenv("DB_PORT", "5432"),
                database=os.getenv("DB_NAME", "basirah"),
                user=os.getenv("DB_USER", "basirah_user"),
                password=os.getenv("DB_PASSWORD", "basirah_secure_password_2025")
            )
            logger.info("Database connection pool initialized")
        except Exception as e:
            logger.error(f"Failed to initialize database pool: {e}")
            raise
    
    @contextmanager
    def get_connection(self):
        """
        Get database connection from pool.
        
        Usage:
            with db.get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT * FROM analyses")
        """
        conn = None
        try:
            conn = self.pool.getconn()
            yield conn
            conn.commit()
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"Database error: {e}")
            raise
        finally:
            if conn:
                self.pool.putconn(conn)
    
    @contextmanager
    def get_cursor(self, cursor_factory=RealDictCursor):
        """
        Get database cursor with automatic commit/rollback.
        
        Usage:
            with db.get_cursor() as cur:
                cur.execute("SELECT * FROM analyses")
                results = cur.fetchall()
        """
        with self.get_connection() as conn:
            cursor = conn.cursor(cursor_factory=cursor_factory)
            try:
                yield cursor
            finally:
                cursor.close()
    
    def execute_query(self, query: str, params: tuple = None) -> list:
        """
        Execute SELECT query and return results.
        
        Args:
            query: SQL query
            params: Query parameters
            
        Returns:
            List of result dictionaries
        """
        with self.get_cursor() as cur:
            cur.execute(query, params)
            return cur.fetchall()
    
    def execute_update(self, query: str, params: tuple = None) -> int:
        """
        Execute INSERT/UPDATE/DELETE query.
        
        Args:
            query: SQL query
            params: Query parameters
            
        Returns:
            Number of affected rows
        """
        with self.get_cursor() as cur:
            cur.execute(query, params)
            return cur.rowcount
    
    def close(self):
        """Close all connections in pool."""
        if self.pool:
            self.pool.closeall()
            logger.info("Database connection pool closed")
    
    def health_check(self) -> bool:
        """
        Check if database is accessible.
        
        Returns:
            True if database is healthy, False otherwise
        """
        try:
            with self.get_cursor() as cur:
                cur.execute("SELECT 1")
                return True
        except Exception as e:
            logger.error(f"Database health check failed: {e}")
            return False


# Singleton instance
_db_manager: Optional[DatabaseManager] = None


def get_db() -> DatabaseManager:
    """Get database manager singleton."""
    global _db_manager
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager


__all__ = ["DatabaseManager", "get_db"]
```

---

## STORAGE SYSTEM IMPLEMENTATION

### File 1: Analysis Storage Manager

**File:** Create new file `src/storage/analysis_storage.py`

```python
"""
Analysis storage system for basƒ´rah.
Stores analyses in both PostgreSQL (for search) and file system (for full content).
"""

import os
import json
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path

from src.storage.database import get_db

logger = logging.getLogger(__name__)


class AnalysisStorage:
    """
    Manages storage of analysis results in both database and file system.
    
    Database: Stores metadata and enables fast search
    File System: Stores complete analysis JSON files
    """
    
    def __init__(self, storage_root: str = "basƒ´rah_analyses"):
        """
        Initialize storage system.
        
        Args:
            storage_root: Root directory for file storage
        """
        self.storage_root = Path(storage_root)
        self.db = get_db()
        self._ensure_directories()
    
    def _ensure_directories(self):
        """Create directory structure if it doesn't exist."""
        directories = [
            self.storage_root / "deep_dive" / "buy",
            self.storage_root / "deep_dive" / "watch",
            self.storage_root / "deep_dive" / "avoid",
            self.storage_root / "quick_screen" / "investigate",
            self.storage_root / "quick_screen" / "pass",
            self.storage_root / "sharia_screen" / "compliant",
            self.storage_root / "sharia_screen" / "doubtful",
            self.storage_root / "sharia_screen" / "non_compliant"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Storage directories ensured at {self.storage_root}")
    
    def save_analysis(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Save analysis to both database and file system.
        
        Args:
            result: Analysis result from agent
            
        Returns:
            Storage metadata (paths, database ID, etc.)
        """
        try:
            # Extract key information
            ticker = result.get('ticker', 'UNKNOWN')
            analysis_type = result.get('metadata', {}).get('analysis_type', 'unknown')
            decision = result.get('decision', 'UNKNOWN')
            analysis_date = datetime.now().date()
            
            # Normalize values
            analysis_type_normalized = self._normalize_analysis_type(analysis_type)
            decision_normalized = self._normalize_decision(decision, analysis_type)
            
            # Generate analysis ID
            years = result.get('metadata', {}).get('years_analyzed', 1)
            analysis_id = self._generate_analysis_id(
                ticker, 
                analysis_date, 
                decision_normalized,
                years if analysis_type_normalized == 'deep_dive' else None
            )
            
            # Determine file path
            file_path = self._get_file_path(
                analysis_type_normalized,
                decision_normalized,
                analysis_id
            )
            
            # Save to file system
            full_path = self.storage_root / file_path
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Saved analysis to file: {full_path}")
            
            # Save to database
            db_id = self._save_to_database(result, analysis_id, str(file_path))
            
            logger.info(f"Saved analysis to database with ID: {db_id}")
            
            return {
                "success": True,
                "analysis_id": analysis_id,
                "database_id": db_id,
                "file_path": str(full_path),
                "relative_path": str(file_path)
            }
            
        except Exception as e:
            logger.error(f"Failed to save analysis: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _normalize_analysis_type(self, analysis_type: str) -> str:
        """Normalize analysis type to standard values."""
        type_map = {
            'quick': 'quick',
            'quick_screen': 'quick',
            'deep_dive': 'deep_dive',
            'deep': 'deep_dive',
            'sharia': 'sharia',
            'sharia_screen': 'sharia',
            'sharia_compliance': 'sharia'
        }
        return type_map.get(analysis_type.lower(), 'unknown')
    
    def _normalize_decision(self, decision: str, analysis_type: str) -> str:
        """Normalize decision to lowercase for consistency."""
        decision_lower = decision.lower()
        
        # Map variations to standard values
        if analysis_type == 'sharia':
            if 'compliant' in decision_lower and 'non' not in decision_lower:
                return 'compliant'
            elif 'doubtful' in decision_lower:
                return 'doubtful'
            else:
                return 'non_compliant'
        elif analysis_type == 'quick':
            if 'investigate' in decision_lower:
                return 'investigate'
            else:
                return 'pass'
        else:  # deep_dive
            if 'buy' in decision_lower:
                return 'buy'
            elif 'watch' in decision_lower:
                return 'watch'
            else:
                return 'avoid'
    
    def _generate_analysis_id(
        self,
        ticker: str,
        date: datetime.date,
        decision: str,
        years: Optional[int] = None
    ) -> str:
        """Generate unique analysis ID."""
        base = f"{ticker}_{date.isoformat()}_{decision}"
        if years and years > 1:
            return f"{base}_{years}y"
        return base
    
    def _get_file_path(
        self,
        analysis_type: str,
        decision: str,
        analysis_id: str
    ) -> Path:
        """Determine file path based on analysis type and decision."""
        return Path(analysis_type) / decision / f"{analysis_id}.json"
    
    def _save_to_database(
        self,
        result: Dict[str, Any],
        analysis_id: str,
        file_path: str
    ) -> int:
        """
        Save analysis metadata to database.
        
        Returns:
            Database ID of saved analysis
        """
        ticker = result.get('ticker', 'UNKNOWN')
        company_name = result.get('company_name', ticker)
        
        # Ensure company exists
        company_id = self._ensure_company_exists(ticker, company_name)
        
        # Extract metadata
        metadata = result.get('metadata', {})
        analysis_type = self._normalize_analysis_type(
            metadata.get('analysis_type', 'unknown')
        )
        decision = result.get('decision', 'UNKNOWN')
        decision_normalized = self._normalize_decision(decision, analysis_type)
        
        # Extract metrics
        conviction = result.get('conviction')
        intrinsic_value = result.get('intrinsic_value')
        current_price = result.get('current_price')
        margin_of_safety = result.get('margin_of_safety')
        
        # Extract financial metrics from metadata or result
        roic = None
        if 'roic' in result:
            roic = result['roic']
        elif 'financial_metrics' in metadata:
            roic = metadata['financial_metrics'].get('roic')
        
        # Sharia metrics
        sharia_status = result.get('status') if analysis_type == 'sharia' else None
        purification_rate = result.get('purification_rate', 0.0) if analysis_type == 'sharia' else None
        
        # Cost and performance
        token_usage = metadata.get('token_usage', {})
        cost = token_usage.get('total_cost', 0.0)
        duration = metadata.get('analysis_duration_seconds', 0)
        
        # Thesis content
        thesis = result.get('thesis', '') or result.get('analysis', '')
        thesis_preview = thesis[:500] if thesis else None
        
        # Insert into database
        query = """
        INSERT INTO analyses (
            analysis_id, company_id, ticker, company_name,
            analysis_type, analysis_date, analysis_datetime, years_analyzed,
            decision, conviction,
            intrinsic_value, current_price, margin_of_safety, roic,
            sharia_status, purification_rate,
            cost, duration_seconds,
            token_usage_input, token_usage_output,
            thesis_preview, thesis_full,
            file_path
        ) VALUES (
            %s, %s, %s, %s,
            %s, %s, %s, %s,
            %s, %s,
            %s, %s, %s, %s,
            %s, %s,
            %s, %s,
            %s, %s,
            %s, %s,
            %s
        ) RETURNING id
        """
        
        params = (
            analysis_id, company_id, ticker, company_name,
            analysis_type, datetime.now().date(), datetime.now(), 
            metadata.get('years_analyzed'),
            decision_normalized, conviction,
            intrinsic_value, current_price, margin_of_safety, roic,
            sharia_status, purification_rate,
            cost, duration,
            token_usage.get('input_tokens'), token_usage.get('output_tokens'),
            thesis_preview, thesis,
            file_path
        )
        
        with self.db.get_cursor() as cur:
            cur.execute(query, params)
            db_id = cur.fetchone()['id']
        
        return db_id
    
    def _ensure_company_exists(self, ticker: str, company_name: str) -> int:
        """
        Ensure company exists in database, create if not.
        
        Returns:
            Company ID
        """
        # Try to find existing company
        query = "SELECT id FROM companies WHERE ticker = %s"
        result = self.db.execute_query(query, (ticker,))
        
        if result:
            return result[0]['id']
        
        # Create new company
        insert_query = """
        INSERT INTO companies (ticker, company_name)
        VALUES (%s, %s)
        RETURNING id
        """
        
        with self.db.get_cursor() as cur:
            cur.execute(insert_query, (ticker, company_name))
            company_id = cur.fetchone()['id']
        
        logger.info(f"Created new company: {ticker} (ID: {company_id})")
        return company_id
    
    def load_analysis(self, analysis_id: str) -> Optional[Dict[str, Any]]:
        """
        Load complete analysis from file system.
        
        Args:
            analysis_id: Analysis ID
            
        Returns:
            Full analysis dictionary or None if not found
        """
        try:
            # Get file path from database
            query = "SELECT file_path FROM analyses WHERE analysis_id = %s"
            result = self.db.execute_query(query, (analysis_id,))
            
            if not result:
                logger.warning(f"Analysis not found: {analysis_id}")
                return None
            
            file_path = self.storage_root / result[0]['file_path']
            
            if not file_path.exists():
                logger.error(f"Analysis file not found: {file_path}")
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        except Exception as e:
            logger.error(f"Failed to load analysis {analysis_id}: {e}")
            return None
    
    def delete_analysis(self, analysis_id: str) -> bool:
        """
        Delete analysis from both database and file system.
        
        Args:
            analysis_id: Analysis ID
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get file path
            query = "SELECT file_path FROM analyses WHERE analysis_id = %s"
            result = self.db.execute_query(query, (analysis_id,))
            
            if not result:
                logger.warning(f"Analysis not found: {analysis_id}")
                return False
            
            file_path = self.storage_root / result[0]['file_path']
            
            # Delete from database
            delete_query = "DELETE FROM analyses WHERE analysis_id = %s"
            self.db.execute_update(delete_query, (analysis_id,))
            
            # Delete file
            if file_path.exists():
                file_path.unlink()
            
            logger.info(f"Deleted analysis: {analysis_id}")
            return True
        
        except Exception as e:
            logger.error(f"Failed to delete analysis {analysis_id}: {e}")
            return False


__all__ = ["AnalysisStorage"]
```

### File 2: Search Engine

**File:** Create new file `src/storage/search_engine.py`

```python
"""
Search engine for basƒ´rah analysis history.
Provides powerful search and filtering capabilities.
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from src.storage.database import get_db

logger = logging.getLogger(__name__)


class AnalysisSearchEngine:
    """
    Search and filter analysis history with multiple criteria.
    """
    
    def __init__(self):
        """Initialize search engine."""
        self.db = get_db()
    
    def search(
        self,
        ticker: Optional[str] = None,
        company_name: Optional[str] = None,
        analysis_types: Optional[List[str]] = None,
        decisions: Optional[List[str]] = None,
        convictions: Optional[List[str]] = None,
        sharia_statuses: Optional[List[str]] = None,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None,
        min_roic: Optional[float] = None,
        min_margin_of_safety: Optional[float] = None,
        max_cost: Optional[float] = None,
        tags: Optional[List[str]] = None,
        sort_by: str = "date",
        sort_order: str = "desc",
        limit: Optional[int] = None,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """
        Search analyses with multiple filters.
        
        Args:
            ticker: Stock ticker (e.g., "AAPL")
            company_name: Company name (fuzzy search)
            analysis_types: List of types: ["quick", "deep_dive", "sharia"]
            decisions: List of decisions: ["BUY", "WATCH", "AVOID", etc.]
            convictions: ["HIGH", "MODERATE", "LOW"]
            sharia_statuses: ["COMPLIANT", "DOUBTFUL", "NON-COMPLIANT"]
            date_from: Start date (YYYY-MM-DD)
            date_to: End date (YYYY-MM-DD)
            min_roic: Minimum ROIC percentage
            min_margin_of_safety: Minimum MoS percentage
            max_cost: Maximum analysis cost
            tags: List of tag names
            sort_by: Field to sort by
            sort_order: "asc" or "desc"
            limit: Maximum results to return
            offset: Number of results to skip
            
        Returns:
            List of matching analyses
        """
        # Build WHERE clauses
        where_clauses = []
        params = []
        
        if ticker:
            where_clauses.append("ticker ILIKE %s")
            params.append(f"%{ticker}%")
        
        if company_name:
            where_clauses.append("company_name ILIKE %s")
            params.append(f"%{company_name}%")
        
        if analysis_types:
            placeholders = ','.join(['%s'] * len(analysis_types))
            where_clauses.append(f"analysis_type IN ({placeholders})")
            params.extend(analysis_types)
        
        if decisions:
            # Normalize decisions to lowercase
            decisions_lower = [d.lower() for d in decisions]
            placeholders = ','.join(['%s'] * len(decisions_lower))
            where_clauses.append(f"LOWER(decision) IN ({placeholders})")
            params.extend(decisions_lower)
        
        if convictions:
            placeholders = ','.join(['%s'] * len(convictions))
            where_clauses.append(f"conviction IN ({placeholders})")
            params.extend(convictions)
        
        if sharia_statuses:
            placeholders = ','.join(['%s'] * len(sharia_statuses))
            where_clauses.append(f"sharia_status IN ({placeholders})")
            params.extend(sharia_statuses)
        
        if date_from:
            where_clauses.append("analysis_date >= %s")
            params.append(date_from)
        
        if date_to:
            where_clauses.append("analysis_date <= %s")
            params.append(date_to)
        
        if min_roic is not None:
            where_clauses.append("roic >= %s")
            params.append(min_roic)
        
        if min_margin_of_safety is not None:
            where_clauses.append("margin_of_safety >= %s")
            params.append(min_margin_of_safety)
        
        if max_cost is not None:
            where_clauses.append("cost <= %s")
            params.append(max_cost)
        
        if tags:
            # Join with tags
            where_clauses.append("""
                id IN (
                    SELECT at.analysis_id 
                    FROM analysis_tags at
                    JOIN tags t ON at.tag_id = t.id
                    WHERE t.name = ANY(%s)
                )
            """)
            params.append(tags)
        
        # Build query
        where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"
        
        # Sort mapping
        sort_map = {
            "date": "analysis_date",
            "ticker": "ticker",
            "cost": "cost",
            "roic": "roic",
            "margin_of_safety": "margin_of_safety",
            "decision": "decision"
        }
        sort_field = sort_map.get(sort_by, "analysis_date")
        sort_direction = "DESC" if sort_order.lower() == "desc" else "ASC"
        
        # Build final query
        query = f"""
        SELECT *
        FROM v_analysis_summary
        WHERE {where_sql}
        ORDER BY {sort_field} {sort_direction}
        """
        
        if limit:
            query += f" LIMIT {limit}"
        if offset:
            query += f" OFFSET {offset}"
        
        results = self.db.execute_query(query, tuple(params))
        return results
    
    def quick_search(self, query: str) -> List[Dict[str, Any]]:
        """
        Quick search by ticker or company name.
        
        Args:
            query: Search query
            
        Returns:
            Matching analyses sorted by date
        """
        sql = """
        SELECT *
        FROM v_analysis_summary
        WHERE ticker ILIKE %s OR company_name ILIKE %s
        ORDER BY analysis_date DESC
        LIMIT 20
        """
        
        search_term = f"%{query}%"
        return self.db.execute_query(sql, (search_term, search_term))
    
    def get_by_ticker(
        self,
        ticker: str,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Get all analyses for a specific ticker.
        
        Args:
            ticker: Stock ticker
            limit: Maximum results
            
        Returns:
            List of analyses for ticker
        """
        query = """
        SELECT *
        FROM v_analysis_summary
        WHERE ticker = %s
        ORDER BY analysis_date DESC
        LIMIT %s
        """
        
        return self.db.execute_query(query, (ticker.upper(), limit))
    
    def get_recent(
        self,
        days: int = 30,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """
        Get recent analyses.
        
        Args:
            days: Number of days back
            limit: Maximum results
            
        Returns:
            Recent analyses
        """
        date_from = (datetime.now() - timedelta(days=days)).date()
        
        query = """
        SELECT *
        FROM v_analysis_summary
        WHERE analysis_date >= %s
        ORDER BY analysis_date DESC
        LIMIT %s
        """
        
        return self.db.execute_query(query, (date_from, limit))
    
    def get_portfolio_candidates(self) -> List[Dict[str, Any]]:
        """
        Get high-conviction BUY decisions.
        
        Returns:
            List of BUY analyses with HIGH conviction
        """
        query = """
        SELECT *
        FROM v_analysis_summary
        WHERE decision = 'buy'
          AND conviction = 'HIGH'
        ORDER BY margin_of_safety DESC
        """
        
        return self.db.execute_query(query)
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about analysis history.
        
        Returns:
            Statistics dictionary
        """
        stats_query = """
        SELECT
            COUNT(*) as total_analyses,
            COUNT(DISTINCT ticker) as unique_companies,
            SUM(cost) as total_cost,
            SUM(duration_seconds) / 3600.0 as total_time_hours,
            MIN(analysis_date) as first_analysis,
            MAX(analysis_date) as last_analysis,
            COUNT(*) FILTER (WHERE analysis_type = 'quick') as quick_screens,
            COUNT(*) FILTER (WHERE analysis_type = 'deep_dive') as deep_dives,
            COUNT(*) FILTER (WHERE analysis_type = 'sharia') as sharia_screens,
            COUNT(*) FILTER (WHERE decision = 'buy') as buy_count,
            COUNT(*) FILTER (WHERE decision = 'watch') as watch_count,
            COUNT(*) FILTER (WHERE decision = 'avoid') as avoid_count,
            COUNT(*) FILTER (WHERE decision = 'investigate') as investigate_count,
            COUNT(*) FILTER (WHERE decision = 'pass') as pass_count,
            COUNT(*) FILTER (WHERE sharia_status = 'COMPLIANT') as compliant_count,
            COUNT(*) FILTER (WHERE sharia_status = 'DOUBTFUL') as doubtful_count,
            COUNT(*) FILTER (WHERE sharia_status = 'NON-COMPLIANT') as non_compliant_count
        FROM analyses
        """
        
        result = self.db.execute_query(stats_query)
        
        if not result:
            return {}
        
        stats = result[0]
        
        # Calculate averages
        if stats['total_analyses'] > 0:
            stats['avg_cost'] = round(stats['total_cost'] / stats['total_analyses'], 2)
        else:
            stats['avg_cost'] = 0
        
        return stats
    
    def full_text_search(self, search_term: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        Search thesis content using full-text search.
        
        Args:
            search_term: Term to search for
            limit: Maximum results
            
        Returns:
            Matching analyses
        """
        query = """
        SELECT *,
               ts_rank(to_tsvector('english', thesis_full), plainto_tsquery('english', %s)) as rank
        FROM v_analysis_summary
        WHERE to_tsvector('english', thesis_full) @@ plainto_tsquery('english', %s)
        ORDER BY rank DESC
        LIMIT %s
        """
        
        return self.db.execute_query(query, (search_term, search_term, limit))


__all__ = ["AnalysisSearchEngine"]
```

---

## UI IMPLEMENTATION

### File: History Browser UI

**File:** Create new file `src/ui/history.py`

```python
"""
Analysis history browser UI for basƒ´rah.
"""

import streamlit as st
from datetime import datetime, timedelta
from typing import List, Dict, Any

from src.storage.analysis_storage import AnalysisStorage
from src.storage.search_engine import AnalysisSearchEngine


def render_history_page():
    """Render the analysis history page."""
    st.title("üìÅ Analysis History")
    
    # Initialize storage and search
    storage = AnalysisStorage()
    search = AnalysisSearchEngine()
    
    # Get statistics
    stats = search.get_statistics()
    
    # Show statistics at top
    render_statistics_summary(stats)
    
    st.divider()
    
    # Search and filter interface
    results = render_search_interface(search)
    
    st.divider()
    
    # Display results
    if results:
        render_results_list(results, storage)
    else:
        st.info("No analyses found matching your criteria.")


def render_statistics_summary(stats: Dict[str, Any]):
    """Render statistics summary."""
    st.subheader("üìä Overview")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Analyses", stats.get('total_analyses', 0))
    
    with col2:
        st.metric("Unique Companies", stats.get('unique_companies', 0))
    
    with col3:
        st.metric("Total Cost", f"${stats.get('total_cost', 0):.2f}")
    
    with col4:
        st.metric("Total Time", f"{stats.get('total_time_hours', 0):.1f}h")
    
    # Show breakdown in expander
    with st.expander("üìà Detailed Statistics", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**By Analysis Type:**")
            st.markdown(f"- Quick Screens: {stats.get('quick_screens', 0)}")
            st.markdown(f"- Deep Dives: {stats.get('deep_dives', 0)}")
            st.markdown(f"- Sharia Screens: {stats.get('sharia_screens', 0)}")
        
        with col2:
            st.markdown("**By Decision:**")
            st.markdown(f"- BUY: {stats.get('buy_count', 0)} ‚≠ê")
            st.markdown(f"- WATCH: {stats.get('watch_count', 0)}")
            st.markdown(f"- AVOID: {stats.get('avoid_count', 0)}")
            st.markdown(f"- INVESTIGATE: {stats.get('investigate_count', 0)}")
            st.markdown(f"- PASS: {stats.get('pass_count', 0)}")
        
        if stats.get('compliant_count', 0) > 0:
            st.markdown("**By Sharia Status:**")
            st.markdown(f"- Compliant: {stats.get('compliant_count', 0)}")
            st.markdown(f"- Doubtful: {stats.get('doubtful_count', 0)}")
            st.markdown(f"- Non-Compliant: {stats.get('non_compliant_count', 0)}")


def render_search_interface(search: AnalysisSearchEngine) -> List[Dict[str, Any]]:
    """Render search and filter interface."""
    st.subheader("üîç Search & Filter")
    
    # Quick search
    col1, col2 = st.columns([3, 1])
    
    with col1:
        quick_search = st.text_input(
            "Quick Search",
            placeholder="Search by ticker or company name...",
            help="Enter ticker (AAPL) or company name (Apple)"
        )
    
    with col2:
        if st.button("Clear Filters", use_container_width=True):
            st.session_state.clear()
            st.rerun()
    
    # Advanced filters
    with st.expander("‚öôÔ∏è Advanced Filters", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**Analysis Type**")
            filter_quick = st.checkbox("Quick Screen", value=True, key="filter_quick")
            filter_deep = st.checkbox("Deep Dive", value=True, key="filter_deep")
            filter_sharia = st.checkbox("Sharia Compliance", value=True, key="filter_sharia")
        
        with col2:
            st.markdown("**Decision**")
            filter_buy = st.checkbox("BUY", value=True, key="filter_buy")
            filter_watch = st.checkbox("WATCH", value=True, key="filter_watch")
            filter_avoid = st.checkbox("AVOID", value=True, key="filter_avoid")
            filter_investigate = st.checkbox("INVESTIGATE", value=True, key="filter_investigate")
            filter_pass = st.checkbox("PASS", value=True, key="filter_pass")
        
        with col3:
            st.markdown("**Date Range**")
            date_preset = st.selectbox(
                "Preset",
                ["All Time", "Last 7 Days", "Last 30 Days", "Last 90 Days", "Custom"],
                key="date_preset"
            )
            
            if date_preset == "Custom":
                date_from = st.date_input("From", key="date_from")
                date_to = st.date_input("To", key="date_to")
            else:
                date_from = None
                date_to = None
        
        # Additional filters
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Financial Metrics**")
            min_roic = st.number_input(
                "Min ROIC (%)",
                min_value=0.0,
                max_value=200.0,
                value=0.0,
                step=5.0,
                key="min_roic"
            )
            
            min_mos = st.number_input(
                "Min Margin of Safety (%)",
                min_value=-50.0,
                max_value=100.0,
                value=-50.0,
                step=5.0,
                key="min_mos"
            )
        
        with col2:
            st.markdown("**Other**")
            conviction_options = st.multiselect(
                "Conviction",
                ["HIGH", "MODERATE", "LOW"],
                default=["HIGH", "MODERATE", "LOW"],
                key="conviction_filter"
            )
            
            sharia_options = st.multiselect(
                "Sharia Status",
                ["COMPLIANT", "DOUBTFUL", "NON-COMPLIANT"],
                default=["COMPLIANT", "DOUBTFUL", "NON-COMPLIANT"],
                key="sharia_filter"
            )
    
    # Sort options
    col1, col2 = st.columns(2)
    
    with col1:
        sort_by = st.selectbox(
            "Sort By",
            ["Date", "Ticker", "Cost", "ROIC", "Margin of Safety"],
            key="sort_by"
        )
    
    with col2:
        sort_order = st.selectbox(
            "Sort Order",
            ["Newest First", "Oldest First"],
            key="sort_order"
        )
    
    # Execute search
    if quick_search:
        # Quick search takes precedence
        results = search.quick_search(quick_search)
    else:
        # Build filter criteria
        analysis_types = []
        if filter_quick:
            analysis_types.append("quick")
        if filter_deep:
            analysis_types.append("deep_dive")
        if filter_sharia:
            analysis_types.append("sharia")
        
        decisions = []
        if filter_buy:
            decisions.append("buy")
        if filter_watch:
            decisions.append("watch")
        if filter_avoid:
            decisions.append("avoid")
        if filter_investigate:
            decisions.append("investigate")
        if filter_pass:
            decisions.append("pass")
        
        # Date range
        if date_preset == "Last 7 Days":
            date_from = (datetime.now() - timedelta(days=7)).date()
            date_to = datetime.now().date()
        elif date_preset == "Last 30 Days":
            date_from = (datetime.now() - timedelta(days=30)).date()
            date_to = datetime.now().date()
        elif date_preset == "Last 90 Days":
            date_from = (datetime.now() - timedelta(days=90)).date()
            date_to = datetime.now().date()
        elif date_preset == "Custom":
            date_from = st.session_state.get('date_from')
            date_to = st.session_state.get('date_to')
        else:
            date_from = None
            date_to = None
        
        # Execute search
        results = search.search(
            analysis_types=analysis_types if analysis_types else None,
            decisions=decisions if decisions else None,
            convictions=conviction_options if conviction_options else None,
            sharia_statuses=sharia_options if sharia_options else None,
            date_from=str(date_from) if date_from else None,
            date_to=str(date_to) if date_to else None,
            min_roic=min_roic if min_roic > 0 else None,
            min_margin_of_safety=min_mos if min_mos > -50 else None,
            sort_by=sort_by.lower().replace(" ", "_"),
            sort_order="desc" if "Newest" in sort_order else "asc"
        )
    
    return results


def render_results_list(results: List[Dict[str, Any]], storage: AnalysisStorage):
    """Render search results list."""
    st.subheader(f"üìã Results ({len(results)} found)")
    
    for result in results:
        render_result_card(result, storage)


def render_result_card(result: Dict[str, Any], storage: AnalysisStorage):
    """Render individual result card."""
    with st.container():
        # Header
        col1, col2, col3 = st.columns([2, 2, 1])
        
        with col1:
            st.markdown(f"### {result['ticker']} - {result['company_name']}")
        
        with col2:
            # Analysis type badge
            type_emoji = {
                'quick': '‚ö°',
                'deep_dive': 'üîç',
                'sharia': '‚ò™Ô∏è'
            }
            emoji = type_emoji.get(result['analysis_type'], 'üìä')
            st.markdown(f"{emoji} **{result['analysis_type'].replace('_', ' ').title()}**")
        
        with col3:
            st.markdown(f"*{result['analysis_date']}*")
        
        # Decision and metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            decision = result['decision'].upper()
            if decision == 'BUY':
                st.success(f"**{decision}**")
            elif decision in ['WATCH', 'INVESTIGATE']:
                st.warning(f"**{decision}**")
            else:
                st.error(f"**{decision}**")
        
        with col2:
            if result.get('conviction'):
                st.markdown(f"**Conviction:** {result['conviction']}")
        
        with col3:
            if result.get('roic'):
                st.markdown(f"**ROIC:** {result['roic']:.1f}%")
        
        with col4:
            if result.get('margin_of_safety'):
                st.markdown(f"**MoS:** {result['margin_of_safety']:.1f}%")
        
        # Thesis preview
        if result.get('thesis_preview'):
            st.markdown(f"*{result['thesis_preview']}...*")
        
        # Action buttons
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("üëÅÔ∏è View Full Analysis", key=f"view_{result['id']}"):
                st.session_state['view_analysis_id'] = result['analysis_id']
                st.session_state['show_analysis_modal'] = True
                st.rerun()
        
        with col2:
            if st.button("üì• Download JSON", key=f"download_{result['id']}"):
                analysis = storage.load_analysis(result['analysis_id'])
                if analysis:
                    st.download_button(
                        "Download",
                        data=str(analysis),
                        file_name=f"{result['analysis_id']}.json",
                        mime="application/json"
                    )
        
        with col3:
            if st.button("üîÑ Re-analyze", key=f"reanalyze_{result['id']}"):
                st.session_state['reanalyze_ticker'] = result['ticker']
                st.switch_page("pages/analysis.py")
        
        with col4:
            if st.button("üóëÔ∏è Delete", key=f"delete_{result['id']}"):
                if storage.delete_analysis(result['analysis_id']):
                    st.success("Analysis deleted")
                    st.rerun()
                else:
                    st.error("Failed to delete analysis")
        
        st.divider()
    
    # Handle view modal
    if st.session_state.get('show_analysis_modal'):
        analysis_id = st.session_state.get('view_analysis_id')
        analysis = storage.load_analysis(analysis_id)
        
        if analysis:
            with st.expander("üìÑ Full Analysis", expanded=True):
                st.markdown(analysis.get('thesis', 'No thesis available'))
                
                if st.button("Close"):
                    st.session_state['show_analysis_modal'] = False
                    st.rerun()


__all__ = ["render_history_page"]
```

---

## INTEGRATION WITH MAIN APP

### Update Main App

**File:** Modify `src/ui/app.py`

**Add at the top (after imports):**

```python
from src.storage.analysis_storage import AnalysisStorage
from src.storage.database import get_db

# Initialize storage
if 'storage' not in st.session_state:
    try:
        st.session_state['storage'] = AnalysisStorage()
    except Exception as e:
        st.sidebar.error(f"Storage initialization failed: {e}")
        st.session_state['storage'] = None

storage = st.session_state.get('storage')
```

**After analysis completes (around line 380):**

```python
# Track cost
if 'token_usage' in result.get('metadata', {}):
    cost = result['metadata']['token_usage']['total_cost']
    if 'session_costs' not in st.session_state:
        st.session_state['session_costs'] = []
    st.session_state['session_costs'].append(cost)

# NEW: Save analysis to history
if storage and result.get('decision') != 'ERROR':
    save_result = storage.save_analysis(result)
    if save_result.get('success'):
        st.success(f"‚úÖ Analysis saved to history: {save_result['analysis_id']}")
    else:
        st.warning(f"‚ö†Ô∏è Failed to save analysis: {save_result.get('error')}")
```

**Add navigation to history in sidebar:**

```python
# In sidebar (after cost tracking)
st.divider()
if st.button("üìÅ View Analysis History", use_container_width=True):
    st.switch_page("pages/history.py")
```

---

## DOCKER SETUP SCRIPT

**File:** Create new file `setup_database.sh`

```bash
#!/bin/bash

# basƒ´rah Database Setup Script
# Sets up PostgreSQL database in Docker

set -e

echo "üöÄ Setting up basƒ´rah database..."

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "‚ùå Error: Docker is not running. Please start Docker first."
    exit 1
fi

# Create .env file if it doesn't exist
if [ ! -f .env ]; then
    echo "üìù Creating .env file..."
    cat > .env << EOF
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=basirah
DB_USER=basirah_user
DB_PASSWORD=basirah_secure_password_2025

# Anthropic API
ANTHROPIC_API_KEY=your_key_here
EOF
    echo "‚úÖ Created .env file. Please update with your API key."
fi

# Start PostgreSQL container
echo "üêò Starting PostgreSQL container..."
docker-compose up -d postgres

# Wait for database to be ready
echo "‚è≥ Waiting for database to be ready..."
sleep 10

# Check health
for i in {1..30}; do
    if docker exec basirah_db pg_isready -U basirah_user -d basirah > /dev/null 2>&1; then
        echo "‚úÖ Database is ready!"
        break
    fi
    echo "Waiting... ($i/30)"
    sleep 2
done

# Install Python dependencies
echo "üì¶ Installing Python dependencies..."
pip install psycopg2-binary

echo ""
echo "‚úÖ Database setup complete!"
echo ""
echo "üìä Database Info:"
echo "  Host: localhost"
echo "  Port: 5432"
echo "  Database: basirah"
echo "  User: basirah_user"
echo ""
echo "üîç To connect:"
echo "  psql -h localhost -U basirah_user -d basirah"
echo ""
echo "üõë To stop:"
echo "  docker-compose down"
echo ""
echo "üóëÔ∏è To remove all data:"
echo "  docker-compose down -v"
echo ""
```

Make executable:
```bash
chmod +x setup_database.sh
```

---

## TESTING PROCEDURES

### Test 1: Database Setup

```bash
# Run setup script
./setup_database.sh

# Verify database is running
docker ps | grep basirah_db

# Test connection
docker exec -it basirah_db psql -U basirah_user -d basirah -c "\dt"

# Should show all tables:
# - companies
# - analyses
# - tags
# - analysis_tags
# - saved_searches
# - comparisons
```

### Test 2: Storage System

```python
# In Python shell or test script
from src.storage.analysis_storage import AnalysisStorage
from src.storage.database import get_db

# Test database connection
db = get_db()
print("Database healthy:", db.health_check())

# Test storage
storage = AnalysisStorage()

# Create mock analysis
mock_result = {
    "ticker": "AAPL",
    "company_name": "Apple Inc.",
    "decision": "BUY",
    "conviction": "HIGH",
    "intrinsic_value": 220,
    "current_price": 180,
    "margin_of_safety": 18.2,
    "thesis": "Apple represents an exceptional...",
    "metadata": {
        "analysis_type": "deep_dive",
        "years_analyzed": 10,
        "token_usage": {
            "input_tokens": 85000,
            "output_tokens": 8000,
            "total_cost": 3.25
        },
        "analysis_duration_seconds": 445
    }
}

# Save analysis
result = storage.save_analysis(mock_result)
print("Saved:", result)

# Load analysis
loaded = storage.load_analysis(result['analysis_id'])
print("Loaded:", loaded['ticker'])
```

### Test 3: Search Functionality

```python
from src.storage.search_engine import AnalysisSearchEngine

search = AnalysisSearchEngine()

# Test quick search
results = search.quick_search("AAPL")
print(f"Found {len(results)} analyses for AAPL")

# Test filtered search
results = search.search(
    decisions=["buy"],
    convictions=["HIGH"],
    min_roic=20
)
print(f"Found {len(results)} high-conviction BUYs with ROIC >20%")

# Get statistics
stats = search.get_statistics()
print("Statistics:", stats)
```

### Test 4: UI Testing

```bash
# Run Streamlit app
streamlit run src/ui/app.py

# Test workflow:
1. Run analysis on AAPL (Deep Dive)
2. Verify "Analysis saved to history" message
3. Click "View Analysis History" in sidebar
4. Search for "AAPL"
5. Filter by "BUY" decision
6. View full analysis
7. Delete analysis
```

---

## SUCCESS CRITERIA

### Phase 6C.1 Complete When:

**Database:**
- [ ] PostgreSQL running in Docker
- [ ] Schema created with all tables
- [ ] Indexes created for fast queries
- [ ] Health check passing

**Storage System:**
- [ ] Analyses save to file system correctly
- [ ] Metadata saves to database
- [ ] Directory structure created automatically
- [ ] File naming convention followed

**Search:**
- [ ] Quick search by ticker works
- [ ] Advanced filters work (type, decision, date)
- [ ] Sort options work
- [ ] Financial metrics filters work
- [ ] Statistics dashboard shows correct data

**UI:**
- [ ] History page displays
- [ ] Search interface functional
- [ ] Results list renders correctly
- [ ] View/Download/Delete actions work
- [ ] Navigation from main app works

**Integration:**
- [ ] Analyses auto-save after completion
- [ ] Success message displays
- [ ] Can view saved analyses
- [ ] No regressions in existing features

---

## FILES SUMMARY

### Files Created (9)

1. **`docker-compose.yml`** - Docker configuration
2. **`db/init/01_create_schema.sql`** - Database schema
3. **`src/storage/database.py`** - Database manager
4. **`src/storage/analysis_storage.py`** - Storage system
5. **`src/storage/search_engine.py`** - Search engine
6. **`src/ui/history.py`** - History browser UI
7. **`setup_database.sh`** - Database setup script
8. **`.env.example`** - Environment template
9. **`requirements.txt`** - Add `psycopg2-binary`

### Files Modified (2)

1. **`src/ui/app.py`**
   - Initialize storage
   - Auto-save after analysis
   - Add history navigation

2. **`.env`**
   - Add database configuration

---

## DEPLOYMENT CHECKLIST

- [ ] Docker installed and running
- [ ] Run `./setup_database.sh`
- [ ] Verify database health
- [ ] Update `.env` with API key
- [ ] Install Python dependencies
- [ ] Test storage system
- [ ] Test search functionality
- [ ] Test UI in Streamlit
- [ ] Run analysis and verify save
- [ ] Search for saved analysis
- [ ] View full analysis
- [ ] Test delete functionality

---

## ESTIMATED TIMELINE

| Task | Time |
|------|------|
| **Database Setup** | |
| - Docker compose config | 15 min |
| - Database schema | 30 min |
| - Database manager | 30 min |
| **Storage System** | |
| - Analysis storage | 90 min |
| - Search engine | 90 min |
| **UI Implementation** | |
| - History browser | 120 min |
| - Integration | 30 min |
| **Testing** | 45 min |
| **Documentation** | 20 min |
| **Total** | **~7 hours** |

---

## NEXT STEPS AFTER 6C.1

**Phase 6C.2: Batch Processing** (builds on this foundation)
- CSV upload interface
- Protocol definitions
- Batch processor with progress tracking
- Summary report generator

**Phase 6C.3: Portfolio Management** (optional)
- Portfolio view (all BUY decisions)
- Re-screening alerts
- Performance tracking
- Position sizing

---

## CONCLUSION

Phase 6C.1 establishes the foundation for professional portfolio management:

**Key Achievements:**
- ‚úÖ PostgreSQL database in Docker
- ‚úÖ Hybrid storage (database + files)
- ‚úÖ Powerful multi-criteria search
- ‚úÖ Beautiful history browser
- ‚úÖ Statistics dashboard
- ‚úÖ Auto-save functionality

**Strategic Value:**
- Foundation for batch processing
- Professional portfolio management
- Long-term analysis tracking
- Investment decision audit trail

**Ready for implementation!** üöÄ

---

*Phase 6C.1: Analysis History with Docker Database & Search*
*Estimated: 7 hours | High Strategic Value*
*Status: Ready for Implementation*
